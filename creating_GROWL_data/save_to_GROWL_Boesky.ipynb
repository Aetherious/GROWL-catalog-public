{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "884c5d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py as h5\n",
    "import matplotlib.pyplot as plt\n",
    "import astropy.units as u\n",
    "from typing import Union, Dict, List, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b72243b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class COMPASDataProcessor:\n",
    "    \"\"\"Class to process COMPAS HDF5 files and extract DCO properties\"\"\"\n",
    "    \n",
    "    def __init__(self, solar_metallicity: float = 0.0142):\n",
    "        self.solar_metallicity = solar_metallicity\n",
    "        \n",
    "    def analytical_star_forming_mass_per_binary_using_kroupa_imf(\n",
    "        self, m1_min: float, m1_max: float, m2_min: float, \n",
    "        fbin: float = 1., imf_mass_bounds: List[float] = [0.01, 0.08, 0.5, 200]\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Analytical computation of the mass of stars formed per binary star formed\n",
    "        using the Kroupa IMF.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        m1_min, m1_max : float\n",
    "            Primary mass range [Msun]\n",
    "        m2_min : float  \n",
    "            Minimum secondary mass [Msun]\n",
    "        fbin : float\n",
    "            Binary fraction\n",
    "        imf_mass_bounds : list\n",
    "            IMF mass boundaries [Msun]\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Mass represented by each binary [Msun]\n",
    "        \"\"\"\n",
    "        m1, m2, m3, m4 = imf_mass_bounds\n",
    "        \n",
    "        if m1_min < m3:\n",
    "            raise ValueError(f\"This analytical derivation requires IMF break m3 < m1_min ({m3} !< {m1_min})\")\n",
    "        \n",
    "        alpha = (-(m4**(-1.3) - m3**(-1.3))/1.3 - \n",
    "                (m3**(-0.3) - m2**(-0.3))/(m3*0.3) + \n",
    "                (m2**0.7 - m1**0.7)/(m2*m3*0.7))**(-1)\n",
    "        \n",
    "        # Average mass of stars\n",
    "        m_avg = alpha * (-(m4**(-0.3) - m3**(-0.3))/0.3 + \n",
    "                        (m3**0.7 - m2**0.7)/(m3*0.7) + \n",
    "                        (m2**1.7 - m1**1.7)/(m2*m3*1.7))\n",
    "        \n",
    "        # Fraction of binaries that COMPAS simulates\n",
    "        fint = (-alpha / 1.3 * (m1_max**(-1.3) - m1_min**(-1.3)) + \n",
    "                alpha * m2_min / 2.3 * (m1_max**(-2.3) - m1_min**(-2.3)))\n",
    "        \n",
    "        # Mass represented by each binary\n",
    "        m_rep = (1/fint) * m_avg * (1.5 + (1-fbin)/fbin)\n",
    "        \n",
    "        return m_rep\n",
    "    \n",
    "    def get_dco_mask(self, \n",
    "                     fdata: h5.File, \n",
    "                     dco_type: str = 'BBH', \n",
    "                     pessimistic: bool = True, \n",
    "                     merges_hubble: bool = True, \n",
    "                     no_RLOF_post_CE: bool = True\n",
    "                    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Create mask for Double Compact Objects of specified type.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        fdata : h5py.File\n",
    "            COMPAS HDF5 file\n",
    "        dco_type : str\n",
    "            Type of DCO: 'BBH', 'BNS', 'NSBH'\n",
    "        pessimistic : bool\n",
    "            pessimistic CE: True, False\n",
    "        merges_hubble : bool\n",
    "            mask merging in a Hubble time: True, False\n",
    "        no_RLOF_post_CE : bool\n",
    "            mask systems with RLOF immediately after CE (assume these are stellar mergers): True, False\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        dco_mask : np.ndarray\n",
    "            Boolean mask for DCOs\n",
    "        \"\"\"\n",
    "        stellar_type1 = fdata['BSE_Double_Compact_Objects']['Stellar_Type(1)'][()]\n",
    "        stellar_type2 = fdata['BSE_Double_Compact_Objects']['Stellar_Type(2)'][()]\n",
    "\n",
    "        \n",
    "        # Pessimistic CE mask        \n",
    "        if pessimistic==True:\n",
    "            optimistic_ce = fdata['BSE_Common_Envelopes']['Optimistic_CE'][()]\n",
    "            pessimistic_ce_mask = np.in1d(\n",
    "                fdata['BSE_Double_Compact_Objects']['SEED'][()], \n",
    "                fdata['BSE_Common_Envelopes']['SEED'][()][optimistic_ce == 0]\n",
    "            )\n",
    "        else: pessimistic_ce_mask = np.repeat(True, len(stellar_type2))\n",
    "        \n",
    "        \n",
    "        if merges_hubble == True: merges_hubble_mask = (fdata['BSE_Double_Compact_Objects']['Merges_Hubble_Time'][()]==1)\n",
    "        else: merges_hubble_mask = np.repeat(True, len(stellar_type2))   \n",
    "        \n",
    "        if no_RLOF_post_CE==True:\n",
    "            rlof_post_ce = fdata['BSE_Common_Envelopes'][\"Immediate_RLOF>CE\"][()]\n",
    "            no_rlof_post_ce_mask = np.in1d(\n",
    "                fdata['BSE_Double_Compact_Objects']['SEED'][()], \n",
    "                fdata['BSE_Common_Envelopes']['SEED'][()][rlof_post_ce == 0]\n",
    "            )\n",
    "        else: no_rlof_post_ce_mask = np.repeat(True, len(stellar_type2))\n",
    "        \n",
    "    \n",
    "            \n",
    "          \n",
    "        \n",
    "        # Define stellar type mappings\n",
    "        type_map = {'NS': 13, 'BH': 14}\n",
    "        \n",
    "        if dco_type == 'BBH':\n",
    "            type_mask = (stellar_type1 == type_map['BH']) & (stellar_type2 == type_map['BH'])\n",
    "        elif dco_type == 'BNS':\n",
    "            type_mask = (stellar_type1 == type_map['NS']) & (stellar_type2 == type_map['NS'])\n",
    "        elif dco_type == 'NSBH':\n",
    "            type_mask = ((stellar_type1 == type_map['NS']) & (stellar_type2 == type_map['BH'])) | \\\n",
    "                       ((stellar_type1 == type_map['BH']) & (stellar_type2 == type_map['NS']))\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown DCO type: {dco_type}\")\n",
    "\n",
    "        dco_mask = type_mask & (merges_hubble_mask == True) & (pessimistic_ce_mask == True) & (no_rlof_post_ce_mask == True)\n",
    "\n",
    "            \n",
    "        return dco_mask\n",
    "    \n",
    "    def get_primary_secondary(self, m1: np.ndarray, m2: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Return (primary, secondary) where primary >= secondary element-wise.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        m1, m2 : np.ndarray\n",
    "            Component masses\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        primary, secondary : np.ndarray\n",
    "            Ordered masses\n",
    "        \"\"\"\n",
    "        primary = np.maximum(m1, m2)\n",
    "        secondary = np.minimum(m1, m2)\n",
    "        return primary, secondary\n",
    "    \n",
    "    def chirp_mass(self, m1: np.ndarray, m2: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute chirp mass from component masses.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        m1, m2 : np.ndarray\n",
    "            Component masses [Msun]\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Chirp masses [Msun]\n",
    "        \"\"\"\n",
    "        return (m1 * m2)**(3/5) / (m1 + m2)**(1/5)\n",
    "    \n",
    "    def process_compas_file(self, file_path: str,\n",
    "                            dco_type: str = 'BBH', \n",
    "                            pessimistic: bool = True,\n",
    "                            merges_hubble: bool = True, \n",
    "                            no_RLOF_post_CE: bool = True\n",
    "                           ) -> DCOParameters:\n",
    "        \"\"\"\n",
    "        Process a COMPAS HDF5 file and extract DCO parameters.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        file_path : str\n",
    "            Path to COMPAS HDF5 file\n",
    "        dco_type : str\n",
    "            Type of DCO to extract\n",
    "        pessimistic : bool\n",
    "            pessimistic CE: True, False\n",
    "        merges_hubble : bool\n",
    "            mask merging in a Hubble time: True, False\n",
    "        no_RLOF_post_CE : bool\n",
    "            mask systems with RLOF immediately after CE (assume these are stellar mergers): True, False\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        DCOParameters\n",
    "            Container with all DCO properties\n",
    "        \"\"\"\n",
    "        with h5.File(file_path, 'r') as fdata:\n",
    "            # Get simulation parameters\n",
    "            initial_mass_min = fdata['Run_Details']['initial-mass-min'][()][0]\n",
    "            initial_mass_max = fdata['Run_Details']['initial-mass-max'][()][0] \n",
    "            minimum_secondary_mass = fdata['Run_Details']['minimum-secondary-mass'][()][0]\n",
    "            \n",
    "            # Calculate mass representation\n",
    "            m_rep_per_binary = self.analytical_star_forming_mass_per_binary_using_kroupa_imf(\n",
    "                m1_min=initial_mass_min, \n",
    "                m1_max=initial_mass_max,\n",
    "                m2_min=minimum_secondary_mass, \n",
    "                fbin=1.0\n",
    "            )\n",
    "            \n",
    "            n_binaries = len(fdata['BSE_System_Parameters']['SEED'][()])\n",
    "            total_mass_evolved = n_binaries * m_rep_per_binary\n",
    "            \n",
    "            # Get DCO mask\n",
    "            dco_mask = self.get_dco_mask(fdata, dco_type, pessimistic, merges_hubble, no_RLOF_post_CE)\n",
    "            n_dcos = np.sum(dco_mask)\n",
    "            \n",
    "            if n_dcos == 0:\n",
    "                print(f\"Warning: No {dco_type} systems found in {file_path}\")\n",
    "                return None\n",
    "            \n",
    "            # Get system parameters for DCOs\n",
    "            mask_sys_dcos = np.in1d(\n",
    "                fdata['BSE_System_Parameters']['SEED'][()], \n",
    "                fdata['BSE_Double_Compact_Objects']['SEED'][()][dco_mask]\n",
    "            )\n",
    "            \n",
    "            # Extract properties\n",
    "            metallicities = fdata['BSE_System_Parameters']['Metallicity@ZAMS(1)'][()][mask_sys_dcos]\n",
    "            mixture_weights = fdata['BSE_Double_Compact_Objects']['mixture_weight'][()][dco_mask]\n",
    "            formation_efficiencies = mixture_weights / total_mass_evolved\n",
    "            \n",
    "            delay_times = (fdata['BSE_Double_Compact_Objects']['Coalescence_Time'][()] + \n",
    "                          fdata['BSE_Double_Compact_Objects']['Time'][()])[dco_mask]\n",
    "            \n",
    "            # Masses\n",
    "            dco_masses_1 = fdata['BSE_Double_Compact_Objects']['Mass(1)'][()][dco_mask]\n",
    "            dco_masses_2 = fdata['BSE_Double_Compact_Objects']['Mass(2)'][()][dco_mask]\n",
    "            primary_masses, secondary_masses = self.get_primary_secondary(dco_masses_1, dco_masses_2)\n",
    "            chirp_masses = self.chirp_mass(dco_masses_1, dco_masses_2)\n",
    "\n",
    "            \n",
    "        return DCOParameters(\n",
    "            metallicities=metallicities,\n",
    "            delay_times=delay_times,\n",
    "            formation_efficiencies=formation_efficiencies,\n",
    "            dco_masses_1=dco_masses_1,\n",
    "            dco_masses_2=dco_masses_2,\n",
    "            primary_masses=primary_masses,\n",
    "            secondary_masses=secondary_masses,\n",
    "            chirp_masses=chirp_masses,\n",
    "            mixture_weights=mixture_weights,\n",
    "            total_mass_evolved=total_mass_evolved,\n",
    "            n_systems=n_dcos\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdc128b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DCOParameters:\n",
    "    \"\"\"Container for Double Compact Object parameters\"\"\"\n",
    "    metallicities: np.ndarray\n",
    "    delay_times: np.ndarray\n",
    "    formation_efficiencies: np.ndarray\n",
    "    dco_masses_1: np.ndarray\n",
    "    dco_masses_2: np.ndarray\n",
    "    primary_masses: np.ndarray\n",
    "    secondary_masses: np.ndarray\n",
    "    chirp_masses: np.ndarray\n",
    "    mixture_weights: np.ndarray\n",
    "    total_mass_evolved: float\n",
    "    n_systems: int\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "\n",
    "        \n",
    "\n",
    "def build_growl_catalog(base_path='/Volumes/GROWL/GROWL_bps'):\n",
    "    \"\"\"\n",
    "    Build a dictionary structure for GROWL catalog with authors and their datasets.\n",
    "    \n",
    "    Structure:\n",
    "    {\n",
    "        'author_name': {\n",
    "            'datasets': ['dataset1', 'dataset2', ...],\n",
    "            'file_name': 'COMPAS_Output_Weighted.h5',\n",
    "            'paths': {\n",
    "                'dataset1': '/Volumes/GROWL/GROWL_bps/Boesky24/alpha0_1beta0_25/',\n",
    "                'dataset2': '/Volumes/GROWL/GROWL_bps/Boesky24/alpha0_1beta0_5/'\n",
    "            }\n",
    "            'labels':{'dataset1': r'$\\alpha 0.1 \\ \\beta=0.25$',\n",
    "                      'dataset2': r'$\\alpha 0.1 \\ \\beta=0.5$'\n",
    "            \n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \"\"\"\n",
    "    catalog = {}\n",
    "    \n",
    "    if not os.path.exists(base_path):\n",
    "        print(f\"Base path {base_path} does not exist\")\n",
    "        return catalog\n",
    "    \n",
    "    # Get all author directories\n",
    "    author_dirs = [d for d in os.listdir(base_path) \n",
    "                  if os.path.isdir(os.path.join(base_path, d)) and not d.startswith('.')]\n",
    "    \n",
    "    for author in author_dirs:\n",
    "        author_path = os.path.join(base_path, author)\n",
    "        \n",
    "        # Get all dataset directories for this author\n",
    "        dataset_dirs = [d for d in os.listdir(author_path) \n",
    "                       if os.path.isdir(os.path.join(author_path, d)) and not d.startswith('.')]\n",
    "        \n",
    "        if not dataset_dirs:\n",
    "            continue\n",
    "            \n",
    "        # Find the common HDF5 file name by checking the first dataset\n",
    "        first_dataset_path = os.path.join(author_path, dataset_dirs[0])\n",
    "        h5_files = glob.glob(os.path.join(first_dataset_path, '*.h5'))\n",
    "        \n",
    "        if not h5_files:\n",
    "            print(f\"Warning: No HDF5 files found in {first_dataset_path}\")\n",
    "            continue\n",
    "            \n",
    "        # Assume the first HDF5 file is the standard one\n",
    "        file_name = os.path.basename(h5_files[0])\n",
    "        \n",
    "        # Build paths dictionary\n",
    "        paths = {}\n",
    "        for dataset in dataset_dirs:\n",
    "            dataset_path = os.path.join(author_path, dataset)\n",
    "            # Verify the HDF5 file exists in this dataset\n",
    "            expected_file = os.path.join(dataset_path, file_name)\n",
    "            if os.path.exists(expected_file):\n",
    "                paths[dataset] = dataset_path + '/'\n",
    "            else:\n",
    "                print(f\"Warning: {expected_file} not found\")\n",
    "        \n",
    "        catalog[author] = {\n",
    "            'datasets': sorted(dataset_dirs),\n",
    "            'file_name': file_name,\n",
    "            'paths': paths\n",
    "        }\n",
    "    \n",
    "    return catalog\n",
    "\n",
    "\n",
    "def process_multiple_models(\n",
    "    catalog: Dict, \n",
    "    author: str, \n",
    "    datasets: List[str], \n",
    "    dco_type: str = 'BBH',\n",
    "    pessimistic: bool = True,\n",
    "    merges_hubble: bool = True,\n",
    "    no_RLOF_post_CE: bool = True\n",
    ") -> Dict[str, DCOParameters]:\n",
    "    \"\"\"\n",
    "    Process multiple COMPAS models for comparison.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    catalog : dict\n",
    "        GROWL catalog dictionary\n",
    "    author : str\n",
    "        Author name\n",
    "    datasets : list\n",
    "        List of dataset names to process\n",
    "    dco_type : str\n",
    "        Type of DCO to extract : 'BBH', 'BHNS', 'BNS'\n",
    "    pessimistic: bool\n",
    "        Assuming Pessimistic Common Envelope CE : True (Pessimistic) or False (Optimistic CE)\n",
    "    merges_hubble : bool\n",
    "        mask merging in a Hubble time: True, False\n",
    "    no_RLOF_post_CE : bool\n",
    "        mask systems with RLOF immediately after CE (assume these are stellar mergers): True, False\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary with dataset names as keys and DCOParameters as values\n",
    "    \"\"\"\n",
    "    processor = COMPASDataProcessor()\n",
    "    results = {}\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        try:\n",
    "            file_path = catalog[author]['paths'][dataset] + catalog[author]['file_name']\n",
    "            print(f\"Processing {author}/{dataset}...\")\n",
    "            \n",
    "            data = processor.process_compas_file(file_path, dco_type, pessimistic, merges_hubble, no_RLOF_post_CE)\n",
    "            if data is not None:\n",
    "                results[dataset] = data\n",
    "                print(f\"  Found {data.n_systems} {dco_type} systems\")\n",
    "            else:\n",
    "                print(f\"  No {dco_type} systems found\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {author}/{dataset}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f24fff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: /Volumes/GROWL/GROWL_bps/Boesky24/alpha10_beta0_75/COMPAS_Output_Weighted.h5 not found\n",
      "Warning: No HDF5 files found in /Volumes/GROWL/GROWL_bps/Romagnolo24/ST_ouput\n",
      "Processing Boesky24/alpha0_1beta0_25...\n",
      "  Found 1649874 BBH systems\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have the GROWL catalog from the previous artifact\n",
    "growl_catalog = build_growl_catalog()\n",
    "\n",
    "# Process multiple Boesky24 models\n",
    "datasets_to_process = ['alpha0_1beta0_25'] #, 'alpha0_1beta0_5', 'alpha0_1beta0_75', 'alpha0_5beta0_25', 'alpha0_5beta0_5', 'alpha0_5beta0_75', 'alpha10_beta0_5'\\\n",
    "#                       , 'alpha10_beta0_75', 'alpha2_beta0_5', 'alpha2beta0_25']  \n",
    "\n",
    "boesky_data = process_multiple_models(growl_catalog, 'Boesky24', datasets_to_process, \n",
    "                                      dco_type='BBH', pessimistic= True, merges_hubble=True, no_RLOF_post_CE=True)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0494ac5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# New directory structure approach:\n",
    "BASE_DIR = '/Volumes/GROWL/GROWL_bps_compact'\n",
    "# Create separate entries for each dataset\n",
    "output_files = create_hdf5_from_boesky_data(boesky_data, BASE_DIR, create_separate_files=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
