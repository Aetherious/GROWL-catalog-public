{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "884c5d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py as h5\n",
    "import matplotlib.pyplot as plt\n",
    "import astropy.units as u\n",
    "from typing import Union, Dict, List, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdcc0b0",
   "metadata": {},
   "source": [
    "### To do \n",
    "\n",
    " - remove `create_separate_files=True` option, dont need this & dont want this. I always want to create a seperate hdf5 file for each entry for now\n",
    " - clean up more \n",
    " - add other (COMPAS) datasets, and make function in seperate python class/folder as functions will be re-used outside of this notebook \n",
    " - add other DCO parameters (& ZAMS parameters): formation channel, SN/CHE, separation (to get spins)\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b72243b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DCOParameters:\n",
    "    \"\"\"Container for Double Compact Object parameters\"\"\"\n",
    "    metallicities: np.ndarray\n",
    "    delay_times: np.ndarray\n",
    "    formation_efficiencies: np.ndarray\n",
    "    dco_masses_1: np.ndarray\n",
    "    dco_masses_2: np.ndarray\n",
    "    primary_masses: np.ndarray\n",
    "    secondary_masses: np.ndarray\n",
    "    chirp_masses: np.ndarray\n",
    "    mixture_weights: np.ndarray\n",
    "    total_mass_evolved: float\n",
    "    n_systems: int\n",
    "\n",
    "\n",
    "        \n",
    "def get_file_path(catalog, author, dataset):\n",
    "    \"\"\"\n",
    "    Get the full path to an HDF5 file for a specific author and dataset.\n",
    "    \"\"\"\n",
    "    if author not in catalog:\n",
    "        raise ValueError(f\"Author '{author}' not found in catalog\")\n",
    "    \n",
    "    if dataset not in catalog[author]['paths']:\n",
    "        raise ValueError(f\"Dataset '{dataset}' not found for author '{author}'\")\n",
    "    \n",
    "    path = catalog[author]['paths'][dataset]\n",
    "    file_name = catalog[author]['file_name']\n",
    "    return os.path.join(path, file_name)\n",
    "\n",
    "def list_authors(catalog):\n",
    "    \"\"\"Get list of all authors.\"\"\"\n",
    "    return list(catalog.keys())\n",
    "\n",
    "def list_datasets(catalog, author):\n",
    "    \"\"\"Get list of all datasets for a specific author.\"\"\"\n",
    "    if author not in catalog:\n",
    "        raise ValueError(f\"Author '{author}' not found in catalog\")\n",
    "    return catalog[author]['datasets']\n",
    "        \n",
    "\n",
    "def print_catalog_summary(catalog):\n",
    "    \"\"\"Print a summary of the catalog structure.\"\"\"\n",
    "    print(\"GROWL Catalog Summary:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for author in sorted(catalog.keys()):\n",
    "        print(f\"\\nAuthor: {author}\")\n",
    "        print(f\"  File: {catalog[author]['file_name']}\")\n",
    "        print(f\"  Datasets ({len(catalog[author]['datasets'])}):\")\n",
    "        for dataset in catalog[author]['datasets']:\n",
    "            print(f\"    - {dataset}\")\n",
    "\n",
    "        \n",
    "\n",
    "def build_growl_catalog(base_path='/Volumes/GROWL/GROWL_bps'):\n",
    "    \"\"\"\n",
    "    Build a dictionary structure for GROWL catalog with authors and their datasets.\n",
    "    \n",
    "    Structure:\n",
    "    {\n",
    "        'author_name': {\n",
    "            'datasets': ['dataset1', 'dataset2', ...],\n",
    "            'file_name': 'COMPAS_Output_Weighted.h5',\n",
    "            'paths': {\n",
    "                'dataset1': '/Volumes/GROWL/GROWL_bps/Boesky24/alpha0_1beta0_25/',\n",
    "                'dataset2': '/Volumes/GROWL/GROWL_bps/Boesky24/alpha0_1beta0_5/'\n",
    "            }\n",
    "            'labels':{'dataset1': r'$\\alpha 0.1 \\ \\beta=0.25$',\n",
    "                      'dataset2': r'$\\alpha 0.1 \\ \\beta=0.5$'\n",
    "            \n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \"\"\"\n",
    "    catalog = {}\n",
    "    \n",
    "    if not os.path.exists(base_path):\n",
    "        print(f\"Base path {base_path} does not exist\")\n",
    "        return catalog\n",
    "    \n",
    "    # Get all author directories\n",
    "    author_dirs = [d for d in os.listdir(base_path) \n",
    "                  if os.path.isdir(os.path.join(base_path, d)) and not d.startswith('.')]\n",
    "    \n",
    "    for author in author_dirs:\n",
    "        author_path = os.path.join(base_path, author)\n",
    "        \n",
    "        # Get all dataset directories for this author\n",
    "        dataset_dirs = [d for d in os.listdir(author_path) \n",
    "                       if os.path.isdir(os.path.join(author_path, d)) and not d.startswith('.')]\n",
    "        \n",
    "        if not dataset_dirs:\n",
    "            continue\n",
    "            \n",
    "        # Find the common HDF5 file name by checking the first dataset\n",
    "        first_dataset_path = os.path.join(author_path, dataset_dirs[0])\n",
    "        h5_files = glob.glob(os.path.join(first_dataset_path, '*.h5'))\n",
    "        \n",
    "        if not h5_files:\n",
    "            print(f\"Warning: No HDF5 files found in {first_dataset_path}\")\n",
    "            continue\n",
    "            \n",
    "        # Assume the first HDF5 file is the standard one\n",
    "        file_name = os.path.basename(h5_files[0])\n",
    "        \n",
    "        # Build paths dictionary\n",
    "        paths = {}\n",
    "        for dataset in dataset_dirs:\n",
    "            dataset_path = os.path.join(author_path, dataset)\n",
    "            # Verify the HDF5 file exists in this dataset\n",
    "            expected_file = os.path.join(dataset_path, file_name)\n",
    "            if os.path.exists(expected_file):\n",
    "                paths[dataset] = dataset_path + '/'\n",
    "            else:\n",
    "                print(f\"Warning: {expected_file} not found\")\n",
    "        \n",
    "        catalog[author] = {\n",
    "            'datasets': sorted(dataset_dirs),\n",
    "            'file_name': file_name,\n",
    "            'paths': paths\n",
    "        }\n",
    "    \n",
    "    return catalog\n",
    "\n",
    "\n",
    "def process_multiple_models(\n",
    "    catalog: Dict, \n",
    "    author: str, \n",
    "    datasets: List[str], \n",
    "    dco_type: str = 'BBH',\n",
    "    pessimistic: bool = True,\n",
    "    merges_hubble: bool = True,\n",
    "    no_RLOF_post_CE: bool = True\n",
    ") -> Dict[str, DCOParameters]:\n",
    "    \"\"\"\n",
    "    Process multiple COMPAS models for comparison.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    catalog : dict\n",
    "        GROWL catalog dictionary\n",
    "    author : str\n",
    "        Author name\n",
    "    datasets : list\n",
    "        List of dataset names to process\n",
    "    dco_type : str\n",
    "        Type of DCO to extract : 'BBH', 'BHNS', 'BNS'\n",
    "    pessimistic: bool\n",
    "        Assuming Pessimistic Common Envelope CE : True (Pessimistic) or False (Optimistic CE)\n",
    "    merges_hubble : bool\n",
    "        mask merging in a Hubble time: True, False\n",
    "    no_RLOF_post_CE : bool\n",
    "        mask systems with RLOF immediately after CE (assume these are stellar mergers): True, False\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary with dataset names as keys and DCOParameters as values\n",
    "    \"\"\"\n",
    "    processor = COMPASDataProcessor()\n",
    "    results = {}\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        try:\n",
    "            file_path = catalog[author]['paths'][dataset] + catalog[author]['file_name']\n",
    "            print(f\"Processing {author}/{dataset}...\")\n",
    "            \n",
    "            data = processor.process_compas_file(file_path, dco_type, pessimistic, merges_hubble, no_RLOF_post_CE)\n",
    "            if data is not None:\n",
    "                results[dataset] = data\n",
    "                print(f\"  Found {data.n_systems} {dco_type} systems\")\n",
    "            else:\n",
    "                print(f\"  No {dco_type} systems found\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {author}/{dataset}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class COMPASDataProcessor:\n",
    "    \"\"\"Class to process COMPAS HDF5 files and extract DCO properties\"\"\"\n",
    "    \n",
    "    def __init__(self, solar_metallicity: float = 0.0142):\n",
    "        self.solar_metallicity = solar_metallicity\n",
    "        \n",
    "    def analytical_star_forming_mass_per_binary_using_kroupa_imf(\n",
    "        self, m1_min: float, m1_max: float, m2_min: float, \n",
    "        fbin: float = 1., imf_mass_bounds: List[float] = [0.01, 0.08, 0.5, 200]\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Analytical computation of the mass of stars formed per binary star formed\n",
    "        using the Kroupa IMF.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        m1_min, m1_max : float\n",
    "            Primary mass range [Msun]\n",
    "        m2_min : float  \n",
    "            Minimum secondary mass [Msun]\n",
    "        fbin : float\n",
    "            Binary fraction\n",
    "        imf_mass_bounds : list\n",
    "            IMF mass boundaries [Msun]\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Mass represented by each binary [Msun]\n",
    "        \"\"\"\n",
    "        m1, m2, m3, m4 = imf_mass_bounds\n",
    "        \n",
    "        if m1_min < m3:\n",
    "            raise ValueError(f\"This analytical derivation requires IMF break m3 < m1_min ({m3} !< {m1_min})\")\n",
    "        \n",
    "        alpha = (-(m4**(-1.3) - m3**(-1.3))/1.3 - \n",
    "                (m3**(-0.3) - m2**(-0.3))/(m3*0.3) + \n",
    "                (m2**0.7 - m1**0.7)/(m2*m3*0.7))**(-1)\n",
    "        \n",
    "        # Average mass of stars\n",
    "        m_avg = alpha * (-(m4**(-0.3) - m3**(-0.3))/0.3 + \n",
    "                        (m3**0.7 - m2**0.7)/(m3*0.7) + \n",
    "                        (m2**1.7 - m1**1.7)/(m2*m3*1.7))\n",
    "        \n",
    "        # Fraction of binaries that COMPAS simulates\n",
    "        fint = (-alpha / 1.3 * (m1_max**(-1.3) - m1_min**(-1.3)) + \n",
    "                alpha * m2_min / 2.3 * (m1_max**(-2.3) - m1_min**(-2.3)))\n",
    "        \n",
    "        # Mass represented by each binary\n",
    "        m_rep = (1/fint) * m_avg * (1.5 + (1-fbin)/fbin)\n",
    "        \n",
    "        return m_rep\n",
    "    \n",
    "    def get_dco_mask(self, \n",
    "                     fdata: h5.File, \n",
    "                     dco_type: str = 'BBH', \n",
    "                     pessimistic: bool = True, \n",
    "                     merges_hubble: bool = True, \n",
    "                     no_RLOF_post_CE: bool = True\n",
    "                    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Create mask for Double Compact Objects of specified type.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        fdata : h5py.File\n",
    "            COMPAS HDF5 file\n",
    "        dco_type : str\n",
    "            Type of DCO: 'BBH', 'BNS', 'NSBH'\n",
    "        pessimistic : bool\n",
    "            pessimistic CE: True, False\n",
    "        merges_hubble : bool\n",
    "            mask merging in a Hubble time: True, False\n",
    "        no_RLOF_post_CE : bool\n",
    "            mask systems with RLOF immediately after CE (assume these are stellar mergers): True, False\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        dco_mask : np.ndarray\n",
    "            Boolean mask for DCOs\n",
    "        \"\"\"\n",
    "        stellar_type1 = fdata['BSE_Double_Compact_Objects']['Stellar_Type(1)'][()]\n",
    "        stellar_type2 = fdata['BSE_Double_Compact_Objects']['Stellar_Type(2)'][()]\n",
    "\n",
    "        \n",
    "        # Pessimistic CE mask        \n",
    "        if pessimistic==True:\n",
    "            optimistic_ce = fdata['BSE_Common_Envelopes']['Optimistic_CE'][()]\n",
    "            pessimistic_ce_mask = np.in1d(\n",
    "                fdata['BSE_Double_Compact_Objects']['SEED'][()], \n",
    "                fdata['BSE_Common_Envelopes']['SEED'][()][optimistic_ce == 0]\n",
    "            )\n",
    "        else: pessimistic_ce_mask = np.repeat(True, len(stellar_type2))\n",
    "        \n",
    "        \n",
    "        if merges_hubble == True: merges_hubble_mask = (fdata['BSE_Double_Compact_Objects']['Merges_Hubble_Time'][()]==1)\n",
    "        else: merges_hubble_mask = np.repeat(True, len(stellar_type2))   \n",
    "        \n",
    "        if no_RLOF_post_CE==True:\n",
    "            rlof_post_ce = fdata['BSE_Common_Envelopes'][\"Immediate_RLOF>CE\"][()]\n",
    "            no_rlof_post_ce_mask = np.in1d(\n",
    "                fdata['BSE_Double_Compact_Objects']['SEED'][()], \n",
    "                fdata['BSE_Common_Envelopes']['SEED'][()][rlof_post_ce == 0]\n",
    "            )\n",
    "        else: no_rlof_post_ce_mask = np.repeat(True, len(stellar_type2))\n",
    "        \n",
    "    \n",
    "            \n",
    "          \n",
    "        \n",
    "        # Define stellar type mappings\n",
    "        type_map = {'NS': 13, 'BH': 14}\n",
    "        \n",
    "        if dco_type == 'BBH':\n",
    "            type_mask = (stellar_type1 == type_map['BH']) & (stellar_type2 == type_map['BH'])\n",
    "        elif dco_type == 'BNS':\n",
    "            type_mask = (stellar_type1 == type_map['NS']) & (stellar_type2 == type_map['NS'])\n",
    "        elif (dco_type == 'NSBH') | (dco_type == 'BHNS'):\n",
    "            type_mask = ((stellar_type1 == type_map['NS']) & (stellar_type2 == type_map['BH'])) | \\\n",
    "                       ((stellar_type1 == type_map['BH']) & (stellar_type2 == type_map['NS']))\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown DCO type: {dco_type}\")\n",
    "\n",
    "        dco_mask = type_mask & (merges_hubble_mask == True) & (pessimistic_ce_mask == True) & (no_rlof_post_ce_mask == True)\n",
    "\n",
    "            \n",
    "        return dco_mask\n",
    "    \n",
    "    def get_primary_secondary(self, m1: np.ndarray, m2: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Return (primary, secondary) where primary >= secondary element-wise.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        m1, m2 : np.ndarray\n",
    "            Component masses\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        primary, secondary : np.ndarray\n",
    "            Ordered masses\n",
    "        \"\"\"\n",
    "        primary = np.maximum(m1, m2)\n",
    "        secondary = np.minimum(m1, m2)\n",
    "        return primary, secondary\n",
    "    \n",
    "    def chirp_mass(self, m1: np.ndarray, m2: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute chirp mass from component masses.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        m1, m2 : np.ndarray\n",
    "            Component masses [Msun]\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Chirp masses [Msun]\n",
    "        \"\"\"\n",
    "        return (m1 * m2)**(3/5) / (m1 + m2)**(1/5)\n",
    "    \n",
    "    def process_compas_file(self, file_path: str,\n",
    "                            dco_type: str = 'BBH', \n",
    "                            pessimistic: bool = True,\n",
    "                            merges_hubble: bool = True, \n",
    "                            no_RLOF_post_CE: bool = True\n",
    "                           ) -> DCOParameters:\n",
    "        \"\"\"\n",
    "        Process a COMPAS HDF5 file and extract DCO parameters.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        file_path : str\n",
    "            Path to COMPAS HDF5 file\n",
    "        dco_type : str\n",
    "            Type of DCO to extract\n",
    "        pessimistic : bool\n",
    "            pessimistic CE: True, False\n",
    "        merges_hubble : bool\n",
    "            mask merging in a Hubble time: True, False\n",
    "        no_RLOF_post_CE : bool\n",
    "            mask systems with RLOF immediately after CE (assume these are stellar mergers): True, False\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        DCOParameters\n",
    "            Container with all DCO properties\n",
    "        \"\"\"\n",
    "        with h5.File(file_path, 'r') as fdata:\n",
    "            # Get simulation parameters\n",
    "            initial_mass_min = fdata['Run_Details']['initial-mass-min'][()][0]\n",
    "            initial_mass_max = fdata['Run_Details']['initial-mass-max'][()][0] \n",
    "            minimum_secondary_mass = fdata['Run_Details']['minimum-secondary-mass'][()][0]\n",
    "            \n",
    "            # Calculate mass representation\n",
    "            m_rep_per_binary = self.analytical_star_forming_mass_per_binary_using_kroupa_imf(\n",
    "                m1_min=initial_mass_min, \n",
    "                m1_max=initial_mass_max,\n",
    "                m2_min=minimum_secondary_mass, \n",
    "                fbin=1.0\n",
    "            )\n",
    "            \n",
    "            n_binaries = len(fdata['BSE_System_Parameters']['SEED'][()])\n",
    "            total_mass_evolved = n_binaries * m_rep_per_binary\n",
    "            \n",
    "            # Get DCO mask\n",
    "            dco_mask = self.get_dco_mask(fdata, dco_type, pessimistic, merges_hubble, no_RLOF_post_CE)\n",
    "            n_dcos = np.sum(dco_mask)\n",
    "            \n",
    "            if n_dcos == 0:\n",
    "                print(f\"Warning: No {dco_type} systems found in {file_path}\")\n",
    "                return None\n",
    "            \n",
    "            # Get system parameters for DCOs\n",
    "            mask_sys_dcos = np.in1d(\n",
    "                fdata['BSE_System_Parameters']['SEED'][()], \n",
    "                fdata['BSE_Double_Compact_Objects']['SEED'][()][dco_mask]\n",
    "            )\n",
    "            \n",
    "            # Extract properties\n",
    "            metallicities = fdata['BSE_System_Parameters']['Metallicity@ZAMS(1)'][()][mask_sys_dcos]\n",
    "            mixture_weights = fdata['BSE_Double_Compact_Objects']['mixture_weight'][()][dco_mask]\n",
    "            formation_efficiencies = mixture_weights / total_mass_evolved\n",
    "            \n",
    "            delay_times = (fdata['BSE_Double_Compact_Objects']['Coalescence_Time'][()] + \n",
    "                          fdata['BSE_Double_Compact_Objects']['Time'][()])[dco_mask]\n",
    "            \n",
    "            # Masses\n",
    "            dco_masses_1 = fdata['BSE_Double_Compact_Objects']['Mass(1)'][()][dco_mask]\n",
    "            dco_masses_2 = fdata['BSE_Double_Compact_Objects']['Mass(2)'][()][dco_mask]\n",
    "            primary_masses, secondary_masses = self.get_primary_secondary(dco_masses_1, dco_masses_2)\n",
    "            chirp_masses = self.chirp_mass(dco_masses_1, dco_masses_2)\n",
    "\n",
    "            \n",
    "        return DCOParameters(\n",
    "            metallicities=metallicities,\n",
    "            delay_times=delay_times,\n",
    "            formation_efficiencies=formation_efficiencies,\n",
    "            dco_masses_1=dco_masses_1,\n",
    "            dco_masses_2=dco_masses_2,\n",
    "            primary_masses=primary_masses,\n",
    "            secondary_masses=secondary_masses,\n",
    "            chirp_masses=chirp_masses,\n",
    "            mixture_weights=mixture_weights,\n",
    "            total_mass_evolved=total_mass_evolved,\n",
    "            n_systems=n_dcos\n",
    "        )\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def create_convolution_hdf5_from_dco_data(\n",
    "    dco_data: Dict[str, DCOParameters], \n",
    "    base_output_dir: str,\n",
    "    author_name: str = \"Boesky24\",\n",
    "    output_filename: str = \"output_example.h5\",\n",
    "    create_separate_files: bool = False\n",
    ") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Create HDF5 file(s) for convolution analysis from processed COMPAS DCO data.\n",
    "    Creates directory structure: base_output_dir/author_name/dataset_name/\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dco_data : Dict[str, DCOParameters]\n",
    "        Dictionary with dataset names as keys and DCOParameters as values\n",
    "        (e.g., result from process_multiple_models)\n",
    "    base_output_dir : str\n",
    "        Base directory (e.g., '/Volumes/GROWL/GROWL_bps_compact')\n",
    "    author_name : str\n",
    "        Author directory name (e.g., 'Boesky24')\n",
    "    output_filename : str\n",
    "        Name of the output HDF5 file\n",
    "    create_separate_files : bool\n",
    "        If True, creates separate HDF5 files for each dataset.\n",
    "        If False, creates one HDF5 file with all datasets.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, str]\n",
    "        Dictionary mapping dataset names to their HDF5 file paths\n",
    "    \"\"\"\n",
    "    output_files = {}\n",
    "    \n",
    "    \n",
    "    \n",
    "    if create_separate_files:\n",
    "        for dataset_name, dco_params in dco_data.items():\n",
    "            output_dir = os.path.join(base_output_dir, author_name, dataset_name)\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            \n",
    "            output_hdf5_filename = os.path.join(output_dir, output_filename)\n",
    "            print(f\"Processing {dataset_name} -> {output_dir}\")\n",
    "            \n",
    "            # Prepare the data dictionary with the required properties\n",
    "            data_dict = {\n",
    "                \"delay_time\": dco_params.delay_times,\n",
    "                \"metallicity\": dco_params.metallicities,\n",
    "                \"formation_efficiency_per_solar_mass\": dco_params.formation_efficiencies,\n",
    "                \"dco_mass_1\": dco_params.dco_masses_1,\n",
    "                \"dco_mass_2\": dco_params.dco_masses_2,\n",
    "            }\n",
    "            \n",
    "            \n",
    "            # Save both data + units\n",
    "            units_dict = {\n",
    "                \"delay_time\": \"Myr\",\n",
    "                \"metallicity\": \"#\",\n",
    "                \"formation_efficiency_per_solar_mass\": \"1/Msun\",\n",
    "                \"dco_mass_1\": \"Msun\",\n",
    "                \"dco_mass_2\": \"Msun\",\n",
    "            }                \n",
    "            \n",
    "\n",
    "            \n",
    "            # Create DataFrame\n",
    "            df = pd.DataFrame.from_records(data_dict)\n",
    "            \n",
    "            # Save to HDF5 under a single clean group\n",
    "            df.to_hdf(output_hdf5_filename, key=\"input_data\", mode='w')  \n",
    "            pd.Series(units_dict).to_hdf(output_hdf5_filename, key=\"units\", mode=\"a\")   \n",
    "\n",
    "            print(f\"  Saved {len(df)} systems with columns: {list(df.columns)}\")\n",
    "            output_files[dataset_name] = output_hdf5_filename\n",
    "            \n",
    "            print(f\"HDF5 file created: {output_hdf5_filename}\")\n",
    "    \n",
    "#     if create_separate_files:\n",
    "#         # Create separate files for each dataset\n",
    "#         for dataset_name, dco_params in dco_data.items():\n",
    "#             # Create directory structure: base_output_dir/author_name/dataset_name/\n",
    "#             output_dir = os.path.join(base_output_dir, author_name, dataset_name)\n",
    "#             os.makedirs(output_dir, exist_ok=True)\n",
    "            \n",
    "#             print(f\"Processing {dataset_name} -> {output_dir}\")\n",
    "            \n",
    "#             # Full path to output file\n",
    "#             output_hdf5_filename = os.path.join(output_dir, output_filename)\n",
    "            \n",
    "#             print(\"creating simple HDF5\")\n",
    "#             with h5.File(output_hdf5_filename, 'w') as f:\n",
    "#                 f.create_group('input_data')\n",
    "            \n",
    "#             # Prepare the data dictionary with the required properties\n",
    "#             data_dict = {\n",
    "#                 \"delay_time\": dco_params.delay_times,\n",
    "#                 \"metallicity\": dco_params.metallicities,\n",
    "#                 \"formation_efficiency_per_solar_mass\": dco_params.formation_efficiencies,\n",
    "#                 \"dco_mass_1\": dco_params.dco_masses_1,  # m1\n",
    "#                 \"dco_mass_2\": dco_params.dco_masses_2,  # m2\n",
    "#             }\n",
    "            \n",
    "#             # Create DataFrame\n",
    "#             df = pd.DataFrame.from_records(data_dict)\n",
    "            \n",
    "#             # Save to HDF5\n",
    "#             df.to_hdf(output_hdf5_filename, key=\"input_data\", mode='a')\n",
    "            \n",
    "#             print(f\"  Saved {len(df)} systems with columns: {list(df.columns)}\")\n",
    "#             output_files[dataset_name] = output_hdf5_filename\n",
    "            \n",
    "#             print(f\"\\nHDF5 file(s) created in: {output_hdf5_filename}\")\n",
    "    \n",
    "    \n",
    "    return output_files\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Convenience function that matches your existing workflow\n",
    "def create_hdf5_from_boesky_data(\n",
    "    boesky_data: Dict[str, DCOParameters],\n",
    "    base_output_dir: str = \"/Volumes/GROWL/GROWL_bps_compact\",\n",
    "    author_name: str = \"Boesky24\",\n",
    "    base_filename: str = \"bps_output_\",\n",
    "    filename_add: str = \"fiducial\",\n",
    "    create_separate_files: bool = True,\n",
    "    dco_type='BBH',\n",
    ") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Create HDF5 file(s) specifically for Boesky data with proper directory structure.\n",
    "    Creates: base_output_dir/Boesky24/dataset_name/ (if separate files)\n",
    "    or: base_output_dir/Boesky24/ (if single file)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    boesky_data : Dict[str, DCOParameters]\n",
    "        Result from process_multiple_models(growl_catalog, 'Boesky24', datasets_to_process)\n",
    "    base_output_dir : str\n",
    "        Base directory path (e.g., '/Volumes/GROWL/GROWL_bps_compact')\n",
    "    author_name : str\n",
    "        Author name (e.g., 'Boesky24')\n",
    "    filename : str\n",
    "        Output filename\n",
    "    create_separate_files : bool\n",
    "        If True, creates separate files in dataset subdirectories\n",
    "        If False, creates single file in author directory\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, str]\n",
    "        Dictionary mapping dataset names to HDF5 file paths\n",
    "    \"\"\"\n",
    "    \n",
    "    filename = base_filename + filename_add + \".h5\"\n",
    "    \n",
    "    return create_convolution_hdf5_from_dco_data(\n",
    "        dco_data=boesky_data,\n",
    "        base_output_dir=base_output_dir,\n",
    "        author_name=author_name,\n",
    "        output_filename=filename,\n",
    "        create_separate_files=create_separate_files\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdc128b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No HDF5 files found in /Volumes/GROWL/GROWL_bps/Romagnolo24/ST_ouput\n",
      "GROWL Catalog Summary:\n",
      "==================================================\n",
      "\n",
      "Author: Boesky24\n",
      "  File: COMPAS_Output_Weighted.h5\n",
      "  Datasets (19):\n",
      "    - alpha0_1beta0_25\n",
      "    - alpha0_1beta0_5\n",
      "    - alpha0_1beta0_75\n",
      "    - alpha0_5beta0_25\n",
      "    - alpha0_5beta0_5\n",
      "    - alpha0_5beta0_75\n",
      "    - alpha10_beta0_25\n",
      "    - alpha10_beta0_5\n",
      "    - alpha10_beta0_75\n",
      "    - alpha2_beta0_5\n",
      "    - alpha2beta0_25\n",
      "    - sigma_265_RMP_Mandel\n",
      "    - sigma_265_RMP_Rapid\n",
      "    - sigma_30_RMP_Delayed\n",
      "    - sigma_30_RMP_Mandel\n",
      "    - sigma_30_RMP_Rapid\n",
      "    - sigma_750_RMP_Delayed\n",
      "    - sigma_750_RMP_Mandel\n",
      "    - sigma_750_RMP_Rapid\n",
      "['alpha0_1beta0_25', 'alpha0_1beta0_5', 'alpha0_1beta0_75', 'alpha0_5beta0_25', 'alpha0_5beta0_5', 'alpha0_5beta0_75', 'alpha10_beta0_25', 'alpha10_beta0_5', 'alpha10_beta0_75', 'alpha2_beta0_5', 'alpha2beta0_25', 'sigma_265_RMP_Mandel', 'sigma_265_RMP_Rapid', 'sigma_30_RMP_Delayed', 'sigma_30_RMP_Mandel', 'sigma_30_RMP_Rapid', 'sigma_750_RMP_Delayed', 'sigma_750_RMP_Mandel', 'sigma_750_RMP_Rapid']\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have the GROWL catalog from the previous artifact\n",
    "growl_catalog = build_growl_catalog()\n",
    "#print GROWL catalog possible entries\n",
    "print_catalog_summary(growl_catalog)\n",
    "\n",
    "boesky_dataset_list =  list_datasets(growl_catalog, 'Boesky24')\n",
    "print(boesky_dataset_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca80cc26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f24fff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Boesky24/alpha0_1beta0_25...\n",
      "  Found 1649874 BBH systems\n",
      "Processing alpha0_1beta0_25 -> /Volumes/GROWL/GROWL_bps_compact/Boesky24/alpha0_1beta0_25\n",
      "  Saved 1649874 systems with columns: ['dco_mass_1', 'dco_mass_2', 'delay_time', 'formation_efficiency_per_solar_mass', 'metallicity']\n",
      "HDF5 file created: /Volumes/GROWL/GROWL_bps_compact/Boesky24/alpha0_1beta0_25/bps_output_BBH_pessimistic.h5\n",
      "\n",
      "Processing Boesky24/alpha0_1beta0_5...\n",
      "  Found 1331908 BBH systems\n",
      "Processing alpha0_1beta0_5 -> /Volumes/GROWL/GROWL_bps_compact/Boesky24/alpha0_1beta0_5\n",
      "  Saved 1331908 systems with columns: ['dco_mass_1', 'dco_mass_2', 'delay_time', 'formation_efficiency_per_solar_mass', 'metallicity']\n",
      "HDF5 file created: /Volumes/GROWL/GROWL_bps_compact/Boesky24/alpha0_1beta0_5/bps_output_BBH_pessimistic.h5\n",
      "\n",
      "Processing Boesky24/alpha0_1beta0_75...\n",
      "  Found 1011769 BBH systems\n",
      "Processing alpha0_1beta0_75 -> /Volumes/GROWL/GROWL_bps_compact/Boesky24/alpha0_1beta0_75\n",
      "  Saved 1011769 systems with columns: ['dco_mass_1', 'dco_mass_2', 'delay_time', 'formation_efficiency_per_solar_mass', 'metallicity']\n",
      "HDF5 file created: /Volumes/GROWL/GROWL_bps_compact/Boesky24/alpha0_1beta0_75/bps_output_BBH_pessimistic.h5\n",
      "\n",
      "Processing Boesky24/alpha0_5beta0_25...\n",
      "  Found 3871867 BBH systems\n",
      "Processing alpha0_5beta0_25 -> /Volumes/GROWL/GROWL_bps_compact/Boesky24/alpha0_5beta0_25\n",
      "  Saved 3871867 systems with columns: ['dco_mass_1', 'dco_mass_2', 'delay_time', 'formation_efficiency_per_solar_mass', 'metallicity']\n",
      "HDF5 file created: /Volumes/GROWL/GROWL_bps_compact/Boesky24/alpha0_5beta0_25/bps_output_BBH_pessimistic.h5\n",
      "\n",
      "Processing Boesky24/alpha0_5beta0_5...\n",
      "  Found 3353619 BBH systems\n",
      "Processing alpha0_5beta0_5 -> /Volumes/GROWL/GROWL_bps_compact/Boesky24/alpha0_5beta0_5\n",
      "  Saved 3353619 systems with columns: ['dco_mass_1', 'dco_mass_2', 'delay_time', 'formation_efficiency_per_solar_mass', 'metallicity']\n",
      "HDF5 file created: /Volumes/GROWL/GROWL_bps_compact/Boesky24/alpha0_5beta0_5/bps_output_BBH_pessimistic.h5\n",
      "\n",
      "Processing Boesky24/alpha0_5beta0_75...\n",
      "  Found 2682498 BBH systems\n",
      "Processing alpha0_5beta0_75 -> /Volumes/GROWL/GROWL_bps_compact/Boesky24/alpha0_5beta0_75\n",
      "  Saved 2682498 systems with columns: ['dco_mass_1', 'dco_mass_2', 'delay_time', 'formation_efficiency_per_solar_mass', 'metallicity']\n",
      "HDF5 file created: /Volumes/GROWL/GROWL_bps_compact/Boesky24/alpha0_5beta0_75/bps_output_BBH_pessimistic.h5\n",
      "\n",
      "Processing Boesky24/alpha10_beta0_25...\n",
      "  Found 2753509 BBH systems\n",
      "Processing alpha10_beta0_25 -> /Volumes/GROWL/GROWL_bps_compact/Boesky24/alpha10_beta0_25\n",
      "  Saved 2753509 systems with columns: ['dco_mass_1', 'dco_mass_2', 'delay_time', 'formation_efficiency_per_solar_mass', 'metallicity']\n",
      "HDF5 file created: /Volumes/GROWL/GROWL_bps_compact/Boesky24/alpha10_beta0_25/bps_output_BBH_pessimistic.h5\n",
      "\n",
      "Processing Boesky24/alpha10_beta0_5...\n",
      "  Found 2825010 BBH systems\n",
      "Processing alpha10_beta0_5 -> /Volumes/GROWL/GROWL_bps_compact/Boesky24/alpha10_beta0_5\n",
      "  Saved 2825010 systems with columns: ['dco_mass_1', 'dco_mass_2', 'delay_time', 'formation_efficiency_per_solar_mass', 'metallicity']\n",
      "HDF5 file created: /Volumes/GROWL/GROWL_bps_compact/Boesky24/alpha10_beta0_5/bps_output_BBH_pessimistic.h5\n",
      "\n",
      "Processing Boesky24/alpha10_beta0_75...\n",
      "  Found 2598648 BBH systems\n",
      "Processing alpha10_beta0_75 -> /Volumes/GROWL/GROWL_bps_compact/Boesky24/alpha10_beta0_75\n",
      "  Saved 2598648 systems with columns: ['dco_mass_1', 'dco_mass_2', 'delay_time', 'formation_efficiency_per_solar_mass', 'metallicity']\n",
      "HDF5 file created: /Volumes/GROWL/GROWL_bps_compact/Boesky24/alpha10_beta0_75/bps_output_BBH_pessimistic.h5\n",
      "\n",
      "Processing Boesky24/alpha2_beta0_5...\n",
      "  Found 3568920 BBH systems\n",
      "Processing alpha2_beta0_5 -> /Volumes/GROWL/GROWL_bps_compact/Boesky24/alpha2_beta0_5\n",
      "  Saved 3568920 systems with columns: ['dco_mass_1', 'dco_mass_2', 'delay_time', 'formation_efficiency_per_solar_mass', 'metallicity']\n",
      "HDF5 file created: /Volumes/GROWL/GROWL_bps_compact/Boesky24/alpha2_beta0_5/bps_output_BBH_pessimistic.h5\n",
      "\n",
      "Processing Boesky24/alpha2beta0_25...\n",
      "  Found 5072599 BBH systems\n",
      "Processing alpha2beta0_25 -> /Volumes/GROWL/GROWL_bps_compact/Boesky24/alpha2beta0_25\n",
      "  Saved 5072599 systems with columns: ['dco_mass_1', 'dco_mass_2', 'delay_time', 'formation_efficiency_per_solar_mass', 'metallicity']\n",
      "HDF5 file created: /Volumes/GROWL/GROWL_bps_compact/Boesky24/alpha2beta0_25/bps_output_BBH_pessimistic.h5\n",
      "\n",
      "Processing Boesky24/sigma_265_RMP_Mandel...\n",
      "  Found 219954 BBH systems\n",
      "Processing sigma_265_RMP_Mandel -> /Volumes/GROWL/GROWL_bps_compact/Boesky24/sigma_265_RMP_Mandel\n",
      "  Saved 219954 systems with columns: ['dco_mass_1', 'dco_mass_2', 'delay_time', 'formation_efficiency_per_solar_mass', 'metallicity']\n",
      "HDF5 file created: /Volumes/GROWL/GROWL_bps_compact/Boesky24/sigma_265_RMP_Mandel/bps_output_BBH_pessimistic.h5\n",
      "\n",
      "Processing Boesky24/sigma_265_RMP_Rapid...\n",
      "  Found 3007976 BBH systems\n",
      "Processing sigma_265_RMP_Rapid -> /Volumes/GROWL/GROWL_bps_compact/Boesky24/sigma_265_RMP_Rapid\n",
      "  Saved 3007976 systems with columns: ['dco_mass_1', 'dco_mass_2', 'delay_time', 'formation_efficiency_per_solar_mass', 'metallicity']\n",
      "HDF5 file created: /Volumes/GROWL/GROWL_bps_compact/Boesky24/sigma_265_RMP_Rapid/bps_output_BBH_pessimistic.h5\n",
      "\n",
      "Processing Boesky24/sigma_30_RMP_Delayed...\n",
      "  Found 3471236 BBH systems\n",
      "Processing sigma_30_RMP_Delayed -> /Volumes/GROWL/GROWL_bps_compact/Boesky24/sigma_30_RMP_Delayed\n",
      "  Saved 3471236 systems with columns: ['dco_mass_1', 'dco_mass_2', 'delay_time', 'formation_efficiency_per_solar_mass', 'metallicity']\n",
      "HDF5 file created: /Volumes/GROWL/GROWL_bps_compact/Boesky24/sigma_30_RMP_Delayed/bps_output_BBH_pessimistic.h5\n",
      "\n",
      "Processing Boesky24/sigma_30_RMP_Mandel...\n",
      "  Found 2401234 BBH systems\n",
      "Processing sigma_30_RMP_Mandel -> /Volumes/GROWL/GROWL_bps_compact/Boesky24/sigma_30_RMP_Mandel\n",
      "  Saved 2401234 systems with columns: ['dco_mass_1', 'dco_mass_2', 'delay_time', 'formation_efficiency_per_solar_mass', 'metallicity']\n",
      "HDF5 file created: /Volumes/GROWL/GROWL_bps_compact/Boesky24/sigma_30_RMP_Mandel/bps_output_BBH_pessimistic.h5\n",
      "\n",
      "Processing Boesky24/sigma_30_RMP_Rapid...\n",
      "  Found 3544628 BBH systems\n",
      "Processing sigma_30_RMP_Rapid -> /Volumes/GROWL/GROWL_bps_compact/Boesky24/sigma_30_RMP_Rapid\n",
      "  Saved 3544628 systems with columns: ['dco_mass_1', 'dco_mass_2', 'delay_time', 'formation_efficiency_per_solar_mass', 'metallicity']\n",
      "HDF5 file created: /Volumes/GROWL/GROWL_bps_compact/Boesky24/sigma_30_RMP_Rapid/bps_output_BBH_pessimistic.h5\n",
      "\n",
      "Processing Boesky24/sigma_750_RMP_Delayed...\n",
      "  Found 2376908 BBH systems\n",
      "Processing sigma_750_RMP_Delayed -> /Volumes/GROWL/GROWL_bps_compact/Boesky24/sigma_750_RMP_Delayed\n",
      "  Saved 2376908 systems with columns: ['dco_mass_1', 'dco_mass_2', 'delay_time', 'formation_efficiency_per_solar_mass', 'metallicity']\n",
      "HDF5 file created: /Volumes/GROWL/GROWL_bps_compact/Boesky24/sigma_750_RMP_Delayed/bps_output_BBH_pessimistic.h5\n",
      "\n",
      "Processing Boesky24/sigma_750_RMP_Mandel...\n",
      "  Found 195762 BBH systems\n",
      "Processing sigma_750_RMP_Mandel -> /Volumes/GROWL/GROWL_bps_compact/Boesky24/sigma_750_RMP_Mandel\n",
      "  Saved 195762 systems with columns: ['dco_mass_1', 'dco_mass_2', 'delay_time', 'formation_efficiency_per_solar_mass', 'metallicity']\n",
      "HDF5 file created: /Volumes/GROWL/GROWL_bps_compact/Boesky24/sigma_750_RMP_Mandel/bps_output_BBH_pessimistic.h5\n",
      "\n",
      "Processing Boesky24/sigma_750_RMP_Rapid...\n",
      "  Found 2545703 BBH systems\n",
      "Processing sigma_750_RMP_Rapid -> /Volumes/GROWL/GROWL_bps_compact/Boesky24/sigma_750_RMP_Rapid\n",
      "  Saved 2545703 systems with columns: ['dco_mass_1', 'dco_mass_2', 'delay_time', 'formation_efficiency_per_solar_mass', 'metallicity']\n",
      "HDF5 file created: /Volumes/GROWL/GROWL_bps_compact/Boesky24/sigma_750_RMP_Rapid/bps_output_BBH_pessimistic.h5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# New directory structure approach:\n",
    "BASE_DIR = '/Volumes/GROWL/GROWL_bps_compact'\n",
    "dco_type='BBH'\n",
    "filename_add = dco_type + '_pessimistic'\n",
    "\n",
    "\n",
    "# Process multiple Boesky24 models\n",
    "for dataset in boesky_dataset_list: \n",
    "    boesky_data = process_multiple_models(growl_catalog, 'Boesky24', [dataset],dco_type=dco_type, pessimistic= True, merges_hubble=True, no_RLOF_post_CE=True)\n",
    "    # Create separate entries for each dataset\n",
    "    output_files = create_hdf5_from_boesky_data(boesky_data, BASE_DIR, create_separate_files=True, filename_add=filename_add)\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0494ac5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Boesky24/alpha0_1beta0_25...\n",
      "  Found 2116 BHNS systems\n",
      "Processing alpha0_1beta0_25 -> /Volumes/GROWL/GROWL_bps_compact/Boesky24/alpha0_1beta0_25\n",
      "  Saved 2116 systems with columns: ['dco_mass_1', 'dco_mass_2', 'delay_time', 'formation_efficiency_per_solar_mass', 'metallicity']\n",
      "HDF5 file created: /Volumes/GROWL/GROWL_bps_compact/Boesky24/alpha0_1beta0_25/bps_output_BHNS_pessimistic.h5\n",
      "\n",
      "Processing Boesky24/alpha0_1beta0_5...\n"
     ]
    }
   ],
   "source": [
    "dco_type='BHNS'\n",
    "filename_add = dco_type + '_pessimistic'\n",
    "for dataset in boesky_dataset_list:\n",
    "    boesky_data = process_multiple_models(growl_catalog, 'Boesky24', [dataset], dco_type=dco_type, pessimistic= True, merges_hubble=True, no_RLOF_post_CE=True)\n",
    "    output_files = create_hdf5_from_boesky_data(boesky_data, BASE_DIR, create_separate_files=True, filename_add=filename_add)\n",
    "    print()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219baee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dco_type='BNS'\n",
    "filename_add = dco_type + '_pessimistic'\n",
    "for dataset in boesky_dataset_list:\n",
    "    boesky_data = process_multiple_models(growl_catalog, 'Boesky24', [dataset], dco_type=dco_type, pessimistic= True, merges_hubble=True, no_RLOF_post_CE=True)\n",
    "    output_files = create_hdf5_from_boesky_data(boesky_data, BASE_DIR, create_separate_files=True, filename_add=filename_add)\n",
    "    print()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acda33c",
   "metadata": {},
   "source": [
    "### Check if it worked by reading in the data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d1639b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "full_path = '/Volumes/GROWL/GROWL_bps_compact/Boesky24/alpha0_1beta0_25/bps_output_BNS_pessimistic.h5'\n",
    "\n",
    "# Read the dataset into a pandas DataFrame\n",
    "df = pd.read_hdf(full_path, key=\"input_data\")\n",
    "\n",
    "# Inspect the available columns\n",
    "print(df.columns)\n",
    "\n",
    "# Extract metallicities\n",
    "metallicities = df[\"metallicity\"].values\n",
    "print(metallicities[:10])   # show first 10 values\n",
    "\n",
    "# read in the units \n",
    "units = pd.read_hdf(full_path, key=\"units\").to_dict()\n",
    "print(units[\"delay_time\"])   # 'Myr'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18dc46d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37463681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# more basic only using hdf5 (but pandas might be better)\n",
    "\n",
    "# def create_convolution_hdf5_from_dco_data(\n",
    "#     dco_data: Dict[str, DCOParameters], \n",
    "#     base_output_dir: str,\n",
    "#     author_name: str = \"Boesky24\",\n",
    "#     output_filename: str = \"output_example.h5\",\n",
    "#     create_separate_files: bool = False\n",
    "# ) -> Dict[str, str]:\n",
    "#     output_files = {}\n",
    "    \n",
    "#     if create_separate_files:\n",
    "#         for dataset_name, dco_params in dco_data.items():\n",
    "#             output_dir = os.path.join(base_output_dir, author_name, dataset_name)\n",
    "#             os.makedirs(output_dir, exist_ok=True)\n",
    "            \n",
    "#             output_hdf5_filename = os.path.join(output_dir, output_filename)\n",
    "#             print(f\"Processing {dataset_name} -> {output_dir}\")\n",
    "            \n",
    "#             # Open HDF5 file for writing\n",
    "#             with h5py.File(output_hdf5_filename, \"w\") as f:\n",
    "#                 grp = f.create_group(\"input_data\")\n",
    "                \n",
    "#                 grp.create_dataset(\"delay_time\", data=dco_params.delay_times)\n",
    "#                 grp.create_dataset(\"metallicity\", data=dco_params.metallicities)\n",
    "#                 grp.create_dataset(\"formation_efficiency_per_solar_mass\", data=dco_params.formation_efficiencies)\n",
    "#                 grp.create_dataset(\"dco_mass_1\", data=dco_params.dco_masses_1)\n",
    "#                 grp.create_dataset(\"dco_mass_2\", data=dco_params.dco_masses_2)\n",
    "            \n",
    "#             print(f\"  Saved {dco_params.n_systems} systems\")\n",
    "#             output_files[dataset_name] = output_hdf5_filename\n",
    "#             print(f\"HDF5 file created: {output_hdf5_filename}\")\n",
    "    \n",
    "#     return output_files"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
