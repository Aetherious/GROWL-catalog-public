{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "884c5d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py as h5\n",
    "import matplotlib.pyplot as plt\n",
    "import astropy.units as u\n",
    "from typing import Union, Dict, List, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "# import glob\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b72243b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DCOParameters:\n",
    "    \"\"\"Container for Double Compact Object parameters\"\"\"\n",
    "    metallicities: np.ndarray\n",
    "    delay_times: np.ndarray\n",
    "    formation_efficiencies: np.ndarray\n",
    "    dco_masses_1: np.ndarray\n",
    "    dco_masses_2: np.ndarray\n",
    "    primary_masses: np.ndarray\n",
    "    secondary_masses: np.ndarray\n",
    "    chirp_masses: np.ndarray\n",
    "    mixture_weights: np.ndarray\n",
    "    total_mass_evolved: float\n",
    "    n_systems: int\n",
    "\n",
    "\n",
    "        \n",
    "def get_file_path(catalog, author, dataset):\n",
    "    \"\"\"\n",
    "    Get the full path to an HDF5 file for a specific author and dataset.\n",
    "    \"\"\"\n",
    "    if author not in catalog:\n",
    "        raise ValueError(f\"Author '{author}' not found in catalog\")\n",
    "    \n",
    "    if dataset not in catalog[author]['paths']:\n",
    "        raise ValueError(f\"Dataset '{dataset}' not found for author '{author}'\")\n",
    "    \n",
    "    path = catalog[author]['paths'][dataset]\n",
    "    file_name = catalog[author]['file_name']\n",
    "    return os.path.join(path, file_name)\n",
    "\n",
    "def list_authors(catalog):\n",
    "    \"\"\"Get list of all authors.\"\"\"\n",
    "    return list(catalog.keys())\n",
    "\n",
    "def list_datasets(catalog, author):\n",
    "    \"\"\"Get list of all datasets for a specific author.\"\"\"\n",
    "    if author not in catalog:\n",
    "        raise ValueError(f\"Author '{author}' not found in catalog\")\n",
    "    return catalog[author]['datasets']\n",
    "        \n",
    "\n",
    "def print_catalog_summary(catalog):\n",
    "    \"\"\"Print a summary of the catalog structure.\"\"\"\n",
    "    print(\"GROWL Catalog Summary:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for author in sorted(catalog.keys()):\n",
    "        print(f\"\\nAuthor: {author}\")\n",
    "        print(f\"  File: {catalog[author]['file_name']}\")\n",
    "        print(f\"  Datasets ({len(catalog[author]['datasets'])}):\")\n",
    "        for dataset in catalog[author]['datasets']:\n",
    "            print(f\"    - {dataset}\")\n",
    "\n",
    "        \n",
    "\n",
    "def build_growl_catalog(base_path='/Volumes/GROWL/GROWL_bps'):\n",
    "    \"\"\"\n",
    "    Build a dictionary structure for GROWL catalog with authors and their datasets.\n",
    "    \n",
    "    Structure:\n",
    "    {\n",
    "        'author_name': {\n",
    "            'datasets': ['dataset1', 'dataset2', ...],\n",
    "            'file_name': 'COMPAS_Output_Weighted.h5',\n",
    "            'paths': {\n",
    "                'dataset1': '/Volumes/GROWL/GROWL_bps/Boesky24/alpha0_1beta0_25/',\n",
    "                'dataset2': '/Volumes/GROWL/GROWL_bps/Boesky24/alpha0_1beta0_5/'\n",
    "            }\n",
    "            'labels':{'dataset1': r'$\\alpha 0.1 \\ \\beta=0.25$',\n",
    "                      'dataset2': r'$\\alpha 0.1 \\ \\beta=0.5$'\n",
    "            \n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \"\"\"\n",
    "    catalog = {}\n",
    "    \n",
    "    if not os.path.exists(base_path):\n",
    "        print(f\"Base path {base_path} does not exist\")\n",
    "        return catalog\n",
    "    \n",
    "    # Get all author directories\n",
    "    author_dirs = [d for d in os.listdir(base_path) \n",
    "                  if os.path.isdir(os.path.join(base_path, d)) and not d.startswith('.')]\n",
    "    \n",
    "    for author in author_dirs:\n",
    "        author_path = os.path.join(base_path, author)\n",
    "        \n",
    "        # Get all dataset directories for this author\n",
    "        dataset_dirs = [d for d in os.listdir(author_path) \n",
    "                       if os.path.isdir(os.path.join(author_path, d)) and not d.startswith('.')]\n",
    "        \n",
    "        if not dataset_dirs:\n",
    "            continue\n",
    "            \n",
    "        # Find the common HDF5 file name by checking the first dataset\n",
    "        first_dataset_path = os.path.join(author_path, dataset_dirs[0])\n",
    "        h5_files = glob.glob(os.path.join(first_dataset_path, '*.h5'))\n",
    "        \n",
    "        if not h5_files:\n",
    "            print(f\"Warning: No HDF5 files found in {first_dataset_path}\")\n",
    "            continue\n",
    "            \n",
    "        # Assume the first HDF5 file is the standard one\n",
    "        file_name = os.path.basename(h5_files[0])\n",
    "        \n",
    "        # Build paths dictionary\n",
    "        paths = {}\n",
    "        for dataset in dataset_dirs:\n",
    "            dataset_path = os.path.join(author_path, dataset)\n",
    "            # Verify the HDF5 file exists in this dataset\n",
    "            expected_file = os.path.join(dataset_path, file_name)\n",
    "            if os.path.exists(expected_file):\n",
    "                paths[dataset] = dataset_path + '/'\n",
    "            else:\n",
    "                print(f\"Warning: {expected_file} not found\")\n",
    "        \n",
    "        catalog[author] = {\n",
    "            'datasets': sorted(dataset_dirs),\n",
    "            'file_name': file_name,\n",
    "            'paths': paths\n",
    "        }\n",
    "    \n",
    "    return catalog\n",
    "\n",
    "\n",
    "def process_multiple_models(\n",
    "    catalog: Dict, \n",
    "    author: str, \n",
    "    datasets: List[str], \n",
    "    dco_type: str = 'BBH',\n",
    "    pessimistic: bool = True,\n",
    "    merges_hubble: bool = True,\n",
    "    no_RLOF_post_CE: bool = True\n",
    ") -> Dict[str, DCOParameters]:\n",
    "    \"\"\"\n",
    "    Process multiple COMPAS models for comparison.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    catalog : dict\n",
    "        GROWL catalog dictionary\n",
    "    author : str\n",
    "        Author name\n",
    "    datasets : list\n",
    "        List of dataset names to process\n",
    "    dco_type : str\n",
    "        Type of DCO to extract : 'BBH', 'BHNS', 'BNS'\n",
    "    pessimistic: bool\n",
    "        Assuming Pessimistic Common Envelope CE : True (Pessimistic) or False (Optimistic CE)\n",
    "    merges_hubble : bool\n",
    "        mask merging in a Hubble time: True, False\n",
    "    no_RLOF_post_CE : bool\n",
    "        mask systems with RLOF immediately after CE (assume these are stellar mergers): True, False\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary with dataset names as keys and DCOParameters as values\n",
    "    \"\"\"\n",
    "    processor = COMPASDataProcessor()\n",
    "    results = {}\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        try:\n",
    "            file_path = catalog[author]['paths'][dataset] + catalog[author]['file_name']\n",
    "            print(f\"Processing {author}/{dataset}...\")\n",
    "            \n",
    "            data = processor.process_compas_file(file_path, dco_type, pessimistic, merges_hubble, no_RLOF_post_CE)\n",
    "            if data is not None:\n",
    "                results[dataset] = data\n",
    "                print(f\"  Found {data.n_systems} {dco_type} systems\")\n",
    "            else:\n",
    "                print(f\"  No {dco_type} systems found\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {author}/{dataset}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class COMPASDataProcessor:\n",
    "    \"\"\"Class to process COMPAS HDF5 files and extract DCO properties\"\"\"\n",
    "    \n",
    "    def __init__(self, solar_metallicity: float = 0.0142):\n",
    "        self.solar_metallicity = solar_metallicity\n",
    "        \n",
    "    def analytical_star_forming_mass_per_binary_using_kroupa_imf(\n",
    "        self, m1_min: float, m1_max: float, m2_min: float, \n",
    "        fbin: float = 1., imf_mass_bounds: List[float] = [0.01, 0.08, 0.5, 200]\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Analytical computation of the mass of stars formed per binary star formed\n",
    "        using the Kroupa IMF.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        m1_min, m1_max : float\n",
    "            Primary mass range [Msun]\n",
    "        m2_min : float  \n",
    "            Minimum secondary mass [Msun]\n",
    "        fbin : float\n",
    "            Binary fraction\n",
    "        imf_mass_bounds : list\n",
    "            IMF mass boundaries [Msun]\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Mass represented by each binary [Msun]\n",
    "        \"\"\"\n",
    "        m1, m2, m3, m4 = imf_mass_bounds\n",
    "        \n",
    "        if m1_min < m3:\n",
    "            raise ValueError(f\"This analytical derivation requires IMF break m3 < m1_min ({m3} !< {m1_min})\")\n",
    "        \n",
    "        alpha = (-(m4**(-1.3) - m3**(-1.3))/1.3 - \n",
    "                (m3**(-0.3) - m2**(-0.3))/(m3*0.3) + \n",
    "                (m2**0.7 - m1**0.7)/(m2*m3*0.7))**(-1)\n",
    "        \n",
    "        # Average mass of stars\n",
    "        m_avg = alpha * (-(m4**(-0.3) - m3**(-0.3))/0.3 + \n",
    "                        (m3**0.7 - m2**0.7)/(m3*0.7) + \n",
    "                        (m2**1.7 - m1**1.7)/(m2*m3*1.7))\n",
    "        \n",
    "        # Fraction of binaries that COMPAS simulates\n",
    "        fint = (-alpha / 1.3 * (m1_max**(-1.3) - m1_min**(-1.3)) + \n",
    "                alpha * m2_min / 2.3 * (m1_max**(-2.3) - m1_min**(-2.3)))\n",
    "        \n",
    "        # Mass represented by each binary\n",
    "        m_rep = (1/fint) * m_avg * (1.5 + (1-fbin)/fbin)\n",
    "        \n",
    "        return m_rep\n",
    "    \n",
    "    def get_dco_mask(self, \n",
    "                     fdata: h5.File, \n",
    "                     dco_type: str = 'BBH', \n",
    "                     pessimistic: bool = True, \n",
    "                     merges_hubble: bool = True, \n",
    "                     no_RLOF_post_CE: bool = True\n",
    "                    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Create mask for Double Compact Objects of specified type.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        fdata : h5py.File\n",
    "            COMPAS HDF5 file\n",
    "        dco_type : str\n",
    "            Type of DCO: 'BBH', 'BNS', 'NSBH'\n",
    "        pessimistic : bool\n",
    "            pessimistic CE: True, False\n",
    "        merges_hubble : bool\n",
    "            mask merging in a Hubble time: True, False\n",
    "        no_RLOF_post_CE : bool\n",
    "            mask systems with RLOF immediately after CE (assume these are stellar mergers): True, False\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        dco_mask : np.ndarray\n",
    "            Boolean mask for DCOs\n",
    "        \"\"\"\n",
    "        stellar_type1 = fdata['BSE_Double_Compact_Objects']['Stellar_Type(1)'][()]\n",
    "        stellar_type2 = fdata['BSE_Double_Compact_Objects']['Stellar_Type(2)'][()]\n",
    "\n",
    "        \n",
    "        # Pessimistic CE mask        \n",
    "        if pessimistic==True:\n",
    "            optimistic_ce = fdata['BSE_Common_Envelopes']['Optimistic_CE'][()]\n",
    "            pessimistic_ce_mask = np.in1d(\n",
    "                fdata['BSE_Double_Compact_Objects']['SEED'][()], \n",
    "                fdata['BSE_Common_Envelopes']['SEED'][()][optimistic_ce == 0]\n",
    "            )\n",
    "        else: pessimistic_ce_mask = np.repeat(True, len(stellar_type2))\n",
    "        \n",
    "        \n",
    "        if merges_hubble == True: merges_hubble_mask = (fdata['BSE_Double_Compact_Objects']['Merges_Hubble_Time'][()]==1)\n",
    "        else: merges_hubble_mask = np.repeat(True, len(stellar_type2))   \n",
    "        \n",
    "        if no_RLOF_post_CE==True:\n",
    "            rlof_post_ce = fdata['BSE_Common_Envelopes'][\"Immediate_RLOF>CE\"][()]\n",
    "            no_rlof_post_ce_mask = np.in1d(\n",
    "                fdata['BSE_Double_Compact_Objects']['SEED'][()], \n",
    "                fdata['BSE_Common_Envelopes']['SEED'][()][rlof_post_ce == 0]\n",
    "            )\n",
    "        else: no_rlof_post_ce_mask = np.repeat(True, len(stellar_type2))\n",
    "        \n",
    "    \n",
    "            \n",
    "          \n",
    "        \n",
    "        # Define stellar type mappings\n",
    "        type_map = {'NS': 13, 'BH': 14}\n",
    "        \n",
    "        if dco_type == 'BBH':\n",
    "            type_mask = (stellar_type1 == type_map['BH']) & (stellar_type2 == type_map['BH'])\n",
    "        elif dco_type == 'BNS':\n",
    "            type_mask = (stellar_type1 == type_map['NS']) & (stellar_type2 == type_map['NS'])\n",
    "        elif dco_type == 'NSBH':\n",
    "            type_mask = ((stellar_type1 == type_map['NS']) & (stellar_type2 == type_map['BH'])) | \\\n",
    "                       ((stellar_type1 == type_map['BH']) & (stellar_type2 == type_map['NS']))\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown DCO type: {dco_type}\")\n",
    "\n",
    "        dco_mask = type_mask & (merges_hubble_mask == True) & (pessimistic_ce_mask == True) & (no_rlof_post_ce_mask == True)\n",
    "\n",
    "            \n",
    "        return dco_mask\n",
    "    \n",
    "    def get_primary_secondary(self, m1: np.ndarray, m2: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Return (primary, secondary) where primary >= secondary element-wise.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        m1, m2 : np.ndarray\n",
    "            Component masses\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        primary, secondary : np.ndarray\n",
    "            Ordered masses\n",
    "        \"\"\"\n",
    "        primary = np.maximum(m1, m2)\n",
    "        secondary = np.minimum(m1, m2)\n",
    "        return primary, secondary\n",
    "    \n",
    "    def chirp_mass(self, m1: np.ndarray, m2: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute chirp mass from component masses.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        m1, m2 : np.ndarray\n",
    "            Component masses [Msun]\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Chirp masses [Msun]\n",
    "        \"\"\"\n",
    "        return (m1 * m2)**(3/5) / (m1 + m2)**(1/5)\n",
    "    \n",
    "    def process_compas_file(self, file_path: str,\n",
    "                            dco_type: str = 'BBH', \n",
    "                            pessimistic: bool = True,\n",
    "                            merges_hubble: bool = True, \n",
    "                            no_RLOF_post_CE: bool = True\n",
    "                           ) -> DCOParameters:\n",
    "        \"\"\"\n",
    "        Process a COMPAS HDF5 file and extract DCO parameters.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        file_path : str\n",
    "            Path to COMPAS HDF5 file\n",
    "        dco_type : str\n",
    "            Type of DCO to extract\n",
    "        pessimistic : bool\n",
    "            pessimistic CE: True, False\n",
    "        merges_hubble : bool\n",
    "            mask merging in a Hubble time: True, False\n",
    "        no_RLOF_post_CE : bool\n",
    "            mask systems with RLOF immediately after CE (assume these are stellar mergers): True, False\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        DCOParameters\n",
    "            Container with all DCO properties\n",
    "        \"\"\"\n",
    "        with h5.File(file_path, 'r') as fdata:\n",
    "            # Get simulation parameters\n",
    "            initial_mass_min = fdata['Run_Details']['initial-mass-min'][()][0]\n",
    "            initial_mass_max = fdata['Run_Details']['initial-mass-max'][()][0] \n",
    "            minimum_secondary_mass = fdata['Run_Details']['minimum-secondary-mass'][()][0]\n",
    "            \n",
    "            # Calculate mass representation\n",
    "            m_rep_per_binary = self.analytical_star_forming_mass_per_binary_using_kroupa_imf(\n",
    "                m1_min=initial_mass_min, \n",
    "                m1_max=initial_mass_max,\n",
    "                m2_min=minimum_secondary_mass, \n",
    "                fbin=1.0\n",
    "            )\n",
    "            \n",
    "            n_binaries = len(fdata['BSE_System_Parameters']['SEED'][()])\n",
    "            total_mass_evolved = n_binaries * m_rep_per_binary\n",
    "            \n",
    "            # Get DCO mask\n",
    "            dco_mask = self.get_dco_mask(fdata, dco_type, pessimistic, merges_hubble, no_RLOF_post_CE)\n",
    "            n_dcos = np.sum(dco_mask)\n",
    "            \n",
    "            if n_dcos == 0:\n",
    "                print(f\"Warning: No {dco_type} systems found in {file_path}\")\n",
    "                return None\n",
    "            \n",
    "            # Get system parameters for DCOs\n",
    "            mask_sys_dcos = np.in1d(\n",
    "                fdata['BSE_System_Parameters']['SEED'][()], \n",
    "                fdata['BSE_Double_Compact_Objects']['SEED'][()][dco_mask]\n",
    "            )\n",
    "            \n",
    "            # Extract properties\n",
    "            metallicities = fdata['BSE_System_Parameters']['Metallicity@ZAMS(1)'][()][mask_sys_dcos]\n",
    "            mixture_weights = fdata['BSE_Double_Compact_Objects']['mixture_weight'][()][dco_mask]\n",
    "            formation_efficiencies = mixture_weights / total_mass_evolved\n",
    "            \n",
    "            delay_times = (fdata['BSE_Double_Compact_Objects']['Coalescence_Time'][()] + \n",
    "                          fdata['BSE_Double_Compact_Objects']['Time'][()])[dco_mask]\n",
    "            \n",
    "            # Masses\n",
    "            dco_masses_1 = fdata['BSE_Double_Compact_Objects']['Mass(1)'][()][dco_mask]\n",
    "            dco_masses_2 = fdata['BSE_Double_Compact_Objects']['Mass(2)'][()][dco_mask]\n",
    "            primary_masses, secondary_masses = self.get_primary_secondary(dco_masses_1, dco_masses_2)\n",
    "            chirp_masses = self.chirp_mass(dco_masses_1, dco_masses_2)\n",
    "\n",
    "            \n",
    "        return DCOParameters(\n",
    "            metallicities=metallicities,\n",
    "            delay_times=delay_times,\n",
    "            formation_efficiencies=formation_efficiencies,\n",
    "            dco_masses_1=dco_masses_1,\n",
    "            dco_masses_2=dco_masses_2,\n",
    "            primary_masses=primary_masses,\n",
    "            secondary_masses=secondary_masses,\n",
    "            chirp_masses=chirp_masses,\n",
    "            mixture_weights=mixture_weights,\n",
    "            total_mass_evolved=total_mass_evolved,\n",
    "            n_systems=n_dcos\n",
    "        )\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def create_convolution_hdf5_from_dco_data(\n",
    "    dco_data: Dict[str, DCOParameters], \n",
    "    base_output_dir: str,\n",
    "    author_name: str = \"Boesky24\",\n",
    "    output_filename: str = \"output_example.h5\",\n",
    "    create_separate_files: bool = False\n",
    ") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Create HDF5 file(s) for convolution analysis from processed COMPAS DCO data.\n",
    "    Creates directory structure: base_output_dir/author_name/dataset_name/\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dco_data : Dict[str, DCOParameters]\n",
    "        Dictionary with dataset names as keys and DCOParameters as values\n",
    "        (e.g., result from process_multiple_models)\n",
    "    base_output_dir : str\n",
    "        Base directory (e.g., '/Volumes/GROWL/GROWL_bps_compact')\n",
    "    author_name : str\n",
    "        Author directory name (e.g., 'Boesky24')\n",
    "    output_filename : str\n",
    "        Name of the output HDF5 file\n",
    "    create_separate_files : bool\n",
    "        If True, creates separate HDF5 files for each dataset.\n",
    "        If False, creates one HDF5 file with all datasets.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, str]\n",
    "        Dictionary mapping dataset names to their HDF5 file paths\n",
    "    \"\"\"\n",
    "    output_files = {}\n",
    "    \n",
    "    if create_separate_files:\n",
    "        # Create separate files for each dataset\n",
    "        for dataset_name, dco_params in dco_data.items():\n",
    "            # Create directory structure: base_output_dir/author_name/dataset_name/\n",
    "            output_dir = os.path.join(base_output_dir, author_name, dataset_name)\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            \n",
    "            print(f\"Processing {dataset_name} -> {output_dir}\")\n",
    "            \n",
    "            # Full path to output file\n",
    "            output_hdf5_filename = os.path.join(output_dir, output_filename)\n",
    "            \n",
    "            print(\"creating simple HDF5\")\n",
    "            with h5.File(output_hdf5_filename, 'w') as f:\n",
    "                f.create_group('input_data')\n",
    "            \n",
    "            # Prepare the data dictionary with the required properties\n",
    "            data_dict = {\n",
    "                \"delay_time\": dco_params.delay_times,\n",
    "                \"metallicity\": dco_params.metallicities,\n",
    "                \"formation_efficiency_per_solar_mass\": dco_params.formation_efficiencies,\n",
    "                \"dco_mass_1\": dco_params.dco_masses_1,  # m1\n",
    "                \"dco_mass_2\": dco_params.dco_masses_2,  # m2\n",
    "            }\n",
    "            \n",
    "            # Create DataFrame\n",
    "            df = pd.DataFrame.from_records(data_dict)\n",
    "            \n",
    "            # Save to HDF5\n",
    "            df.to_hdf(output_hdf5_filename, key=\"input_data/example\", mode='a')\n",
    "            \n",
    "            print(f\"  Saved {len(df)} systems with columns: {list(df.columns)}\")\n",
    "            output_files[dataset_name] = output_hdf5_filename\n",
    "            \n",
    "    else:\n",
    "        # Create single file with all datasets in author directory\n",
    "        output_dir = os.path.join(base_output_dir, author_name)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        output_hdf5_filename = os.path.join(output_dir, output_filename)\n",
    "        \n",
    "\n",
    "        print(\"creating simple HDF5\")\n",
    "        with h5.File(output_hdf5_filename, 'w') as f:\n",
    "            f.create_group('input_data')\n",
    "        \n",
    "        # Process each dataset\n",
    "        for dataset_name, dco_params in dco_data.items():\n",
    "            print(f\"Adding {dataset_name} to HDF5...\")\n",
    "            \n",
    "            # Prepare the data dictionary with the required properties\n",
    "            data_dict = {\n",
    "                \"delay_time\": dco_params.delay_times,\n",
    "                \"metallicity\": dco_params.metallicities,\n",
    "                \"number_per_solar_mass_values\": dco_params.formation_efficiencies,\n",
    "                \"mass_1\": dco_params.primary_masses,  # m1\n",
    "                \"mass_2\": dco_params.secondary_masses,  # m2\n",
    "            }\n",
    "            \n",
    "            \n",
    "            # Create DataFrame\n",
    "            df = pd.DataFrame.from_records(data_dict)\n",
    "            \n",
    "            # Save to HDF5 with dataset-specific key\n",
    "            hdf5_key = f\"input_data/{dataset_name}\"\n",
    "            df.to_hdf(output_hdf5_filename, key=hdf5_key, mode='a')\n",
    "            \n",
    "            print(f\"  Saved {len(df)} systems with columns: {list(df.columns)}\")\n",
    "        \n",
    "        # Add single file to output dictionary for all datasets\n",
    "        for dataset_name in dco_data.keys():\n",
    "            output_files[dataset_name] = output_hdf5_filename\n",
    "    \n",
    "    print(f\"\\nHDF5 file(s) created in: {output_hdf5_filename}\")\n",
    "    return output_files\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Convenience function that matches your existing workflow\n",
    "def create_hdf5_from_boesky_data(\n",
    "    boesky_data: Dict[str, DCOParameters],\n",
    "    base_output_dir: str = \"/Volumes/GROWL/GROWL_bps_compact\",\n",
    "    author_name: str = \"Boesky24\",\n",
    "    base_filename: str = \"bps_output_\",\n",
    "    filename_add: str = \"fiducial\",\n",
    "    create_separate_files: bool = True,\n",
    "    dco_type='BBH',\n",
    ") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Create HDF5 file(s) specifically for Boesky data with proper directory structure.\n",
    "    Creates: base_output_dir/Boesky24/dataset_name/ (if separate files)\n",
    "    or: base_output_dir/Boesky24/ (if single file)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    boesky_data : Dict[str, DCOParameters]\n",
    "        Result from process_multiple_models(growl_catalog, 'Boesky24', datasets_to_process)\n",
    "    base_output_dir : str\n",
    "        Base directory path (e.g., '/Volumes/GROWL/GROWL_bps_compact')\n",
    "    author_name : str\n",
    "        Author name (e.g., 'Boesky24')\n",
    "    filename : str\n",
    "        Output filename\n",
    "    create_separate_files : bool\n",
    "        If True, creates separate files in dataset subdirectories\n",
    "        If False, creates single file in author directory\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, str]\n",
    "        Dictionary mapping dataset names to HDF5 file paths\n",
    "    \"\"\"\n",
    "    \n",
    "    filename = base_filename + filename_add + \".h5\"\n",
    "    \n",
    "    return create_convolution_hdf5_from_dco_data(\n",
    "        dco_data=boesky_data,\n",
    "        base_output_dir=base_output_dir,\n",
    "        author_name=author_name,\n",
    "        output_filename=filename,\n",
    "        create_separate_files=create_separate_files\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdc128b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No HDF5 files found in /Volumes/GROWL/GROWL_bps/Romagnolo24/ST_ouput\n",
      "GROWL Catalog Summary:\n",
      "==================================================\n",
      "\n",
      "Author: Boesky24\n",
      "  File: COMPAS_Output_Weighted.h5\n",
      "  Datasets (20):\n",
      "    - alpha0_1beta0_25\n",
      "    - alpha0_1beta0_5\n",
      "    - alpha0_1beta0_75\n",
      "    - alpha0_5beta0_25\n",
      "    - alpha0_5beta0_5\n",
      "    - alpha0_5beta0_75\n",
      "    - alpha10_beta0_25\n",
      "    - alpha10_beta0_5\n",
      "    - alpha10_beta0_75\n",
      "    - alpha2_beta0_5\n",
      "    - alpha2beta0_25\n",
      "    - sigma_265_RMP_Mandel\n",
      "    - sigma_265_RMP_Rapid\n",
      "    - sigma_265_RMS_Mandel\n",
      "    - sigma_30_RMP_Delayed\n",
      "    - sigma_30_RMP_Rapid\n",
      "    - sigma_30_RMS_Mandel\n",
      "    - sigma_750_RMP_Rapid\n",
      "    - sigma_750_RMS_Delayed\n",
      "    - sigma_750_RMS_Mandel\n",
      "['alpha0_1beta0_25', 'alpha0_1beta0_5', 'alpha0_1beta0_75', 'alpha0_5beta0_25', 'alpha0_5beta0_5', 'alpha0_5beta0_75', 'alpha10_beta0_25', 'alpha10_beta0_5', 'alpha10_beta0_75', 'alpha2_beta0_5', 'alpha2beta0_25', 'sigma_265_RMP_Mandel', 'sigma_265_RMP_Rapid', 'sigma_265_RMS_Mandel', 'sigma_30_RMP_Delayed', 'sigma_30_RMP_Rapid', 'sigma_30_RMS_Mandel', 'sigma_750_RMP_Rapid', 'sigma_750_RMS_Delayed', 'sigma_750_RMS_Mandel']\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have the GROWL catalog from the previous artifact\n",
    "growl_catalog = build_growl_catalog()\n",
    "#print GROWL catalog possible entries\n",
    "print_catalog_summary(growl_catalog)\n",
    "\n",
    "boesky_dataset_list =  list_datasets(growl_catalog, 'Boesky24')\n",
    "print(boesky_dataset_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a1458a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f24fff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Boesky24/alpha0_1beta0_25...\n",
      "  Found 1649874 BBH systems\n",
      "Processing alpha0_1beta0_25 -> /Volumes/GROWL/GROWL_bps_compact/Boesky24/alpha0_1beta0_25\n",
      "creating simple HDF5\n",
      "  Saved 1649874 systems with columns: ['dco_mass_1', 'dco_mass_2', 'delay_time', 'formation_efficiency_per_solar_mass', 'metallicity']\n",
      "\n",
      "HDF5 file(s) created in: /Volumes/GROWL/GROWL_bps_compact/Boesky24/alpha0_1beta0_25/bps_output_BBH_pessimistic.h5\n",
      "Processing Boesky24/alpha0_1beta0_5...\n",
      "  Found 1331908 BBH systems\n",
      "Processing alpha0_1beta0_5 -> /Volumes/GROWL/GROWL_bps_compact/Boesky24/alpha0_1beta0_5\n",
      "creating simple HDF5\n",
      "  Saved 1331908 systems with columns: ['dco_mass_1', 'dco_mass_2', 'delay_time', 'formation_efficiency_per_solar_mass', 'metallicity']\n",
      "\n",
      "HDF5 file(s) created in: /Volumes/GROWL/GROWL_bps_compact/Boesky24/alpha0_1beta0_5/bps_output_BBH_pessimistic.h5\n",
      "Processing Boesky24/alpha0_1beta0_75...\n",
      "  Found 1011769 BBH systems\n",
      "Processing alpha0_1beta0_75 -> /Volumes/GROWL/GROWL_bps_compact/Boesky24/alpha0_1beta0_75\n",
      "creating simple HDF5\n",
      "  Saved 1011769 systems with columns: ['dco_mass_1', 'dco_mass_2', 'delay_time', 'formation_efficiency_per_solar_mass', 'metallicity']\n",
      "\n",
      "HDF5 file(s) created in: /Volumes/GROWL/GROWL_bps_compact/Boesky24/alpha0_1beta0_75/bps_output_BBH_pessimistic.h5\n",
      "Processing Boesky24/alpha0_5beta0_25...\n",
      "  Found 3871867 BBH systems\n",
      "Processing alpha0_5beta0_25 -> /Volumes/GROWL/GROWL_bps_compact/Boesky24/alpha0_5beta0_25\n",
      "creating simple HDF5\n",
      "  Saved 3871867 systems with columns: ['dco_mass_1', 'dco_mass_2', 'delay_time', 'formation_efficiency_per_solar_mass', 'metallicity']\n",
      "\n",
      "HDF5 file(s) created in: /Volumes/GROWL/GROWL_bps_compact/Boesky24/alpha0_5beta0_25/bps_output_BBH_pessimistic.h5\n",
      "Processing Boesky24/alpha0_5beta0_5...\n",
      "  Found 3353619 BBH systems\n",
      "Processing alpha0_5beta0_5 -> /Volumes/GROWL/GROWL_bps_compact/Boesky24/alpha0_5beta0_5\n",
      "creating simple HDF5\n",
      "  Saved 3353619 systems with columns: ['dco_mass_1', 'dco_mass_2', 'delay_time', 'formation_efficiency_per_solar_mass', 'metallicity']\n",
      "\n",
      "HDF5 file(s) created in: /Volumes/GROWL/GROWL_bps_compact/Boesky24/alpha0_5beta0_5/bps_output_BBH_pessimistic.h5\n",
      "Processing Boesky24/alpha0_5beta0_75...\n",
      "  Found 2682498 BBH systems\n",
      "Processing alpha0_5beta0_75 -> /Volumes/GROWL/GROWL_bps_compact/Boesky24/alpha0_5beta0_75\n",
      "creating simple HDF5\n",
      "  Saved 2682498 systems with columns: ['dco_mass_1', 'dco_mass_2', 'delay_time', 'formation_efficiency_per_solar_mass', 'metallicity']\n",
      "\n",
      "HDF5 file(s) created in: /Volumes/GROWL/GROWL_bps_compact/Boesky24/alpha0_5beta0_75/bps_output_BBH_pessimistic.h5\n",
      "Processing Boesky24/alpha10_beta0_25...\n"
     ]
    }
   ],
   "source": [
    "# New directory structure approach:\n",
    "BASE_DIR = '/Volumes/GROWL/GROWL_bps_compact'\n",
    "\n",
    "# Process multiple Boesky24 models\n",
    "# datasets_to_process = boesky_dataset_list #['alpha0_1beta0_25']  \n",
    "\n",
    "\n",
    "for dataset in boesky_dataset_list:\n",
    "    print()\n",
    "\n",
    "    boesky_data = process_multiple_models(growl_catalog, 'Boesky24', [dataset], \n",
    "                                          dco_type='BBH', pessimistic= True, merges_hubble=True, no_RLOF_post_CE=True)\n",
    "\n",
    "    # Create separate entries for each dataset\n",
    "    output_files = create_hdf5_from_boesky_data(boesky_data, BASE_DIR, create_separate_files=True, filename_add='BBH_pessimistic')\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0494ac5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in boesky_dataset_list:\n",
    "    print()\n",
    "    boesky_data = process_multiple_models(growl_catalog, 'Boesky24', [dataset], \n",
    "                                          dco_type='BHNS', pessimistic= True, merges_hubble=True, no_RLOF_post_CE=True)\n",
    "\n",
    "    # Create separate entries for each dataset\n",
    "    output_files = create_hdf5_from_boesky_data(boesky_data, BASE_DIR, create_separate_files=True, filename_add='BBH_pessimistic')\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee863b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in boesky_dataset_list:\n",
    "    print()\n",
    "    boesky_data = process_multiple_models(growl_catalog, 'Boesky24', [dataset], \n",
    "                                          dco_type='BNS', pessimistic= True, merges_hubble=True, no_RLOF_post_CE=True)\n",
    "\n",
    "    # Create separate entries for each dataset\n",
    "    output_files = create_hdf5_from_boesky_data(boesky_data, BASE_DIR, create_separate_files=True, filename_add='BBH_pessimistic')\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
